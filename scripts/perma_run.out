wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: ryanjakecherian (ryanjakecherian-oxford-university). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /home/ryanc/bnn/scripts/wandb/run-20250305_010128-w68kvfxe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MNIST flattened test-2025-03-05T01:01:27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ryanjakecherian-oxford-university/train_classifier
wandb: üöÄ View run at https://wandb.ai/ryanjakecherian-oxford-university/train_classifier/runs/w68kvfxe
[[36m2025-03-05 01:01:29,512[0m][[34m__main__[0m][[32mINFO[0m] - save_dir: /users/ryanc/tmp/test/MNIST flattened test-2025-03-05T01:01:27[0m
[[36m2025-03-05 01:01:29,513[0m][[34m__main__[0m][[32mINFO[0m] - (MNIST flattened test-2025-03-05T01:01:27) - saving schema[0m
epoch: 0
batch_id: 0
loss: 2144.50244140625
batch_id: 50
loss: 901.6407470703125
batch_id: 100
loss: 679.9598388671875
batch_id: 150
loss: 982.561767578125
batch_id: 200
loss: 872.3850708007812
batch_id: 250
loss: 620.0881958007812
batch_id: 300
loss: 697.0440673828125
batch_id: 350
loss: 654.960693359375
batch_id: 400
loss: 509.17279052734375
batch_id: 450
loss: 574.149169921875
epoch: 1
batch_id: 0
loss: 364.13226318359375
batch_id: 50
loss: 480.1121520996094
batch_id: 100
loss: 513.0382690429688
batch_id: 150
loss: 489.7038269042969
batch_id: 200
loss: 535.6694946289062
batch_id: 250
loss: 507.19122314453125
batch_id: 300
loss: 527.427001953125
batch_id: 350
loss: 332.6518859863281
batch_id: 400
loss: 406.9188232421875
batch_id: 450
loss: 417.8109130859375
epoch: 2
batch_id: 0
loss: 366.27691650390625
batch_id: 50
loss: 328.690673828125
batch_id: 100
loss: 357.40093994140625
batch_id: 150
loss: 308.84161376953125
batch_id: 200
loss: 389.07781982421875
batch_id: 250
loss: 394.1585998535156
batch_id: 300
loss: 325.5747985839844
batch_id: 350
loss: 298.69683837890625
batch_id: 400
loss: 372.21002197265625
batch_id: 450
loss: 261.88311767578125
epoch: 3
batch_id: 0
loss: 267.11102294921875
batch_id: 50
loss: 339.1514587402344
batch_id: 100
loss: 310.5330810546875
batch_id: 150
loss: 342.3168029785156
batch_id: 200
loss: 269.5404052734375
batch_id: 250
loss: 258.23388671875
batch_id: 300
loss: 279.6177062988281
batch_id: 350
loss: 292.94622802734375
batch_id: 400
loss: 275.8109436035156
batch_id: 450
loss: 283.4993896484375
epoch: 4
batch_id: 0
loss: 295.993408203125
batch_id: 50
loss: 254.45370483398438
batch_id: 100
loss: 250.62179565429688
batch_id: 150
loss: 237.1693115234375
batch_id: 200
loss: 248.43092346191406
batch_id: 250
loss: 240.9384765625
batch_id: 300
loss: 244.97898864746094
batch_id: 350
loss: 228.51040649414062
batch_id: 400
loss: 253.84890747070312
batch_id: 450
loss: 243.25384521484375
epoch: 5
batch_id: 0
loss: 237.80674743652344
batch_id: 50
loss: 276.8136291503906
batch_id: 100
loss: 261.3746032714844
batch_id: 150
loss: 240.7745361328125
batch_id: 200
loss: 230.5362091064453
batch_id: 250
loss: 238.42666625976562
batch_id: 300
loss: 244.24465942382812
batch_id: 350
loss: 225.4659881591797
batch_id: 400
loss: 239.96517944335938
batch_id: 450
loss: 232.4200897216797
epoch: 6
batch_id: 0
loss: 231.1498565673828
batch_id: 50
loss: 270.5910949707031
batch_id: 100
loss: 243.9698486328125
batch_id: 150
loss: 220.58554077148438
batch_id: 200
loss: 223.2424774169922
batch_id: 250
loss: 219.31817626953125
batch_id: 300
loss: 230.7530517578125
batch_id: 350
loss: 233.503662109375
batch_id: 400
loss: 229.03591918945312
batch_id: 450
loss: 215.75845336914062
epoch: 7
batch_id: 0
loss: 214.99327087402344
batch_id: 50
loss: 228.15927124023438
batch_id: 100
loss: 231.64044189453125
batch_id: 150
loss: 236.57513427734375
batch_id: 200
loss: 225.33035278320312
batch_id: 250
loss: 223.66188049316406
batch_id: 300
loss: 225.90792846679688
batch_id: 350
loss: 234.23226928710938
batch_id: 400
loss: 204.7877655029297
batch_id: 450
loss: 203.603515625
epoch: 8
batch_id: 0
loss: 202.15652465820312
batch_id: 50
loss: 203.55130004882812
batch_id: 100
loss: 197.24488830566406
batch_id: 150
loss: 202.50225830078125
batch_id: 200
loss: 210.98043823242188
batch_id: 250
loss: 224.68211364746094
batch_id: 300
loss: 226.62026977539062
batch_id: 350
loss: 227.21421813964844
batch_id: 400
loss: 209.59197998046875
batch_id: 450
loss: 212.75778198242188
epoch: 9
batch_id: 0
loss: 241.30642700195312
batch_id: 50
loss: 209.80032348632812
batch_id: 100
loss: 185.16519165039062
batch_id: 150
loss: 202.0733642578125
batch_id: 200
loss: 220.64183044433594
batch_id: 250
loss: 195.40951538085938
batch_id: 300
loss: 194.37884521484375
batch_id: 350
loss: 216.68197631835938
batch_id: 400
loss: 213.5727081298828
batch_id: 450
loss: 197.84832763671875
epoch: 10
batch_id: 0
loss: 187.87261962890625
batch_id: 50
loss: 195.09991455078125
batch_id: 100
loss: 234.3369598388672
batch_id: 150
loss: 219.18521118164062
batch_id: 200
loss: 222.02633666992188
batch_id: 250
loss: 217.39599609375
batch_id: 300
loss: 221.46142578125
batch_id: 350
loss: 210.69894409179688
batch_id: 400
loss: 229.76739501953125
batch_id: 450
loss: 207.55018615722656
epoch: 11
batch_id: 0
loss: 191.10415649414062
batch_id: 50
loss: 209.909423828125
batch_id: 100
loss: 216.29522705078125
batch_id: 150
loss: 197.40963745117188
batch_id: 200
loss: 206.77572631835938
batch_id: 250
loss: 196.53323364257812
batch_id: 300
loss: 206.4044952392578
batch_id: 350
loss: 203.25595092773438
batch_id: 400
loss: 204.83743286132812
batch_id: 450
loss: 216.17642211914062
epoch: 12
batch_id: 0
loss: 201.57907104492188
batch_id: 50
loss: 196.85775756835938
batch_id: 100
loss: 200.45350646972656
batch_id: 150
loss: 197.94473266601562
batch_id: 200
loss: 208.7552490234375
batch_id: 250
loss: 197.50128173828125
batch_id: 300
loss: 214.81381225585938
batch_id: 350
loss: 211.10877990722656
batch_id: 400
loss: 209.89215087890625
batch_id: 450
loss: 211.33206176757812
epoch: 13
batch_id: 0
loss: 214.04910278320312
batch_id: 50
loss: 195.70407104492188
batch_id: 100
loss: 205.98486328125
batch_id: 150
loss: 213.47348022460938
batch_id: 200
loss: 217.62033081054688
batch_id: 250
loss: 208.3174591064453
batch_id: 300
loss: 181.03457641601562
batch_id: 350
loss: 195.73007202148438
batch_id: 400
loss: 193.36761474609375
batch_id: 450
loss: 199.63339233398438
epoch: 14
batch_id: 0
loss: 204.136962890625
batch_id: 50
loss: 198.02020263671875
batch_id: 100
loss: 186.187255859375
batch_id: 150
loss: 193.34046936035156
batch_id: 200
loss: 199.69667053222656
batch_id: 250
loss: 215.76406860351562
batch_id: 300
loss: 209.52337646484375
batch_id: 350
loss: 193.6802978515625
batch_id: 400
loss: 220.65103149414062
batch_id: 450
loss: 193.5675048828125
[1;34mwandb[0m: üöÄ View run [33mMNIST flattened test-2025-03-05T01:01:27[0m at: [34mhttps://wandb.ai/ryanjakecherian-oxford-university/train_classifier/runs/w68kvfxe[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../home/ryanc/bnn/scripts/wandb/run-20250305_010128-w68kvfxe/logs[0m
