hidden_dims: [256, 256, 256, 256, 256]

forward_func_default:
  _target_: bnn.functions.forward.reluBinarise
forward_func_last:
  _target_: bnn.functions.forward.relu_noBinarise  #outputs logits in N, s.t. softmax is effectively applied in the loss function

backward_func_default:
  _target_: bnn.functions.backward.reluBackward
backward_func_last:
  _target_: bnn.functions.backward.reluBackward


model:
  _target_: bnn.network.TernBinNetwork
  dims: ${sandwich_list:${dataset.input_dim},${network.hidden_dims},${dataset.output_dim}}
  forward_func:
    - ${network.forward_func_default}
    - ${network.forward_func_last}
  backward_func:
    - ${network.backward_func_default}
    - ${network.backward_func_last}
